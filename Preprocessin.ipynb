{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Devex_train.csv',encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencode = df[['Label 1', 'Label 2', 'Label 3',\\\n",
    "    'Label 4', 'Label 5', 'Label 6',\\\n",
    "    'Label 7','Label 8', 'Label 9', \\\n",
    "    'Label 10', 'Label 11', 'Label 12']].applymap(str).applymap(lambda x: x.split('-')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_frequency_table(data):\n",
    "    frequencytable = {}\n",
    "    for key in data:\n",
    "        #print(key)\n",
    "        for element in data[key]:\n",
    "            if element in frequencytable:\n",
    "                #print(\"Hello\")\n",
    "                #print(key)\n",
    "                frequencytable[element] += 1\n",
    "            else:\n",
    "                frequencytable[element] = 1\n",
    "    return frequencytable\n",
    "D = to_frequency_table(onehotencode)\n",
    "del D['nan']\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def hot_encode_free_text(df,col_list):\n",
    "    df1 = df[col_list].apply(lambda x: ' '.join(x), axis=1)\n",
    "    #cv = CountVectorizer(vocabulary=vocab,ngram_range=(1, 2))#token_pattern=r'(?u)\\b\\w+\\b\n",
    "    #cv = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')#\\b[^\\d\\W]+\\b\n",
    "    cv = CountVectorizer(tokenizer=my_tokenizer)\n",
    "    X_train = cv.fit_transform(df)\n",
    "    X_train = pd.DataFrame(X_train.toarray(), columns=cv.get_feature_names())\n",
    "    return X_train.applymap(convert_num)\n",
    "    #return X_train\n",
    "def convert_num(num):\n",
    "    if num == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\\b[^\\d\\W]+\\b\n",
    "import spacy\n",
    "#from html import unescape\n",
    "#from spacy.lang.en.examples import sentences\n",
    "# create a spaCy tokenizer\n",
    "spacy.load('en')\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "def my_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens])\n",
    "#my_tokenizer(vocab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['3.1.1','3.1.2',\n",
    " '3.2.1','3.2.2','3.3.1',\n",
    "'3.3.2','3.3.3','3.3.4',\n",
    "'3.3.5','3.4.1','3.4.2',\n",
    "'3.5.1','3.5.2','3.6.1',\n",
    "'3.7.1','3.7.2','3.8.1',\n",
    "'3.8.2','3.9.1','3.9.2',\n",
    "'3.9.3','3.a.1','3.b.1',\n",
    "'3.b.2','3.b.3','3.c.1',\n",
    " '3.d.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.max_rows = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_encoded_labels = hot_encode_free_text(onehotencode, onehotencode.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(train[['Unique ID', 'Type', 'Text']], hot_encoded_labels,\\\n",
    "                         left_on=train[['Unique ID', 'Type', 'Text']].index,\\\n",
    "                         right_on=hot_encoded_labels.index, how ='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Without removing stop words\n",
    "\n",
    "def clean_text_final(text):\n",
    "    \n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # remove the characters [\\], ['] and [\"]\n",
    "    text = re.sub(r\"\\\\\", \"\", text)    \n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)    \n",
    "    \n",
    "    # convert text to lowercase\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    # replace punctuation characters with spaces\n",
    "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data.at[i,'Text'] = clean_text_final(data.at[i,'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = data.rename(columns={'Unique ID':'ID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.drop(['Type'], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
